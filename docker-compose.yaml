version: '3.8'

services:
  inferV1:
    image: nvcr.io/nvidia/tritonserver:24.05-pyt-python-py3
    user: triton-server
    command:
      - sh
      - -c
      - |
        tritonserver \
        --model-repository=/models \
        --log-format=ISO8601 \
        --model-control-mode explicit \
        --load-model fasttext_language_identification \
        --load-model sentencex \
        --load-model seamlessm4t_text2text \
        --load-model translate
    shm_size: "1gb"
    volumes:
      - ./model_repository:/models
      - /home/$USER/.cache/huggingface/hub:/hub
    ports:
      - 8000:8000
      - 8001:8001
      - 8002:8002
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
