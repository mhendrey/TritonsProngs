version: '3.8'

services:
  inferV1:
    image: nvcr.io/nvidia/tritonserver:24.05-pyt-python-py3
    command: sh -c "tritonserver --model-repository=/models"
    shm_size: "1gb"
    volumes:
      - ./model_repository:/models
      - /home/$USER/.cache/huggingface/hub:/hub
    ports:
      - 8000:8000
      - 8001:8001
      - 8002:8002
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
